import chromadb 
from sentence_transformers import SentenceTransformer
import os
import sys
import json
import re

class RetrievalPipeline:
    def __init__(self, db_path="project/chroma_db"):
        # Initialise le mod√®le SentenceTransformer pour les embeddings de texte
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        # Cr√©e ou connecte une base de donn√©es Chroma persistante au chemin donn√©
        self.chroma_client = chromadb.PersistentClient(db_path)
        # R√©cup√®re ou cr√©e une collection dans la base appel√©e "law_text"
        self.collection = self.chroma_client.get_or_create_collection(name="law_text")
    
    def find_category(self, text):
        # trouve la categorie aproximatif
        
        # recupere le dic dans base_dechets.json
        with open("project\\base_dechets.json", mode="r", encoding="utf-8") as f:
            category = json.load(f)
        # cree un dic avec les meme cle que l original
        dominent_category = {key:0 for key in category}
        # parcour chaque cle de category
        for key,data in category.items():
            # recupere les poid de chaque categorie
            weight = data["weight"]
            total = 0
            for word in data["keywords"]:
                #pour chaque partie de texte conte l'aparition des mot en ajoutent le poid
                pattern = r"\b" + word + r"\b"
                total += len(re.findall(pattern, text))
                dominent_category[key] = total * weight
        
        # la categorie qui apparait le plus
        dominent = max(dominent_category, key=dominent_category.get)
        # return la categorie
        return dominent

    def chunking(self, text, chunk_size=450, overlap=50):
        # Divise un texte long en petits segments qui se chevauchent pour une meilleure qualit√© d‚Äôembedding
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            # D√©place la fen√™tre vers l‚Äôavant, en gardant un chevauchement pour pr√©server le contexte
            start += chunk_size - overlap
        # verifie si les chunk ne sont pas trop petit et donc sens context
        for i, chunk in enumerate(chunks):
            if len(chunk) > 300:
                chunks.pop(i)

        return chunks

    def index_text(self, file_path):
        # Lit le contenu du fichier texte en encodage UTF-8
        with open(file_path, "r", encoding='utf-8') as text:
            text_law = text.read()

        # Divise le texte en segments
        chunks = self.chunking(text_law)
        # R√©cup√®re le nom du fichier (sans extension) pour l‚Äôutiliser comme identifiant unique
        file_id = os.path.splitext(os.path.basename(file_path))[0]
        
        #essaye de recuperer la date
        pattern = r"(janv|fevr|mars|avr|mai|juin|juil|aout|sept|oct|nov|dec)[\s\-]+[0-9]{4}"
        # essaye de trouve une date au debut du texte
        match = re.search(pattern, text_law[:100], re.IGNORECASE)
        # si match = True return la date recupere
        if match:
            date = match.group(0)
        else:
            date="unknow"
        # R√©cup√®re les identifiants de documents existants dans la collection Chroma pour √©viter les doublons
        existing_ids = set(self.collection.get()["ids"])
        new_chunks = 0

        # Boucle sur tous les segments du fichier
        for i, chunk in enumerate(chunks):
            # Cr√©e un identifiant unique pour chaque segment bas√© sur le nom du fichier et son index
            chunk_id = f"{file_id}_chunk_{i}" 
            # recupere la categorie
            category = self.find_category(chunk)
            # Passe ce segment s‚Äôil est d√©j√† index√©
            if chunk_id in existing_ids:
                continue
            
            # G√©n√®re un embedding pour le segment √† l‚Äôaide du mod√®le
            embedding = self.model.encode(chunk, convert_to_numpy=True)
            # Ajoute le segment, son embedding et ses m√©tadonn√©es (chemin du fichier) √† la collection
            self.collection.add(
                ids=[chunk_id],
                documents=[chunk],
                embeddings=[embedding],
                metadatas=[{"source": file_path, "categorie": category, "date": date}]
            )
            new_chunks += 1

        # Affiche combien de nouveaux segments ont √©t√© index√©s
        if new_chunks > 0:
            print(f"{new_chunks} New chunk indexed from {file_path}")
    
    def query_search(self, query_text):
        # V√©rifie que la requ√™te n‚Äôest pas vide
        if not query_text or query_text.strip() == "":
            return None
        
        # Encode le texte de la requ√™te en un vecteur d‚Äôembedding
        query_embedding = self.model.encode(query_text)
        # Recherche dans la collection Chroma les segments les plus similaires
        
        # R√©cup√®re la liste des documents dans le dossier
        clean_data_path_list = os.listdir("./project/clean_data")
        # La quantit√© de documents 
        n_result = len(clean_data_path_list)
        
        result = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_result
        )
        # Retourne les r√©sultats de la recherche
        return result
    
    def display_results(self, query, results):
        """Affiche les r√©sultats de recherche de mani√®re claire et format√©e"""
        # V√©rifier si les r√©sultats sont valides
        if results is None:
            print("\n" + "="*100)
            print(" ‚ö†Ô∏è  ERREUR ".center(100))
            print("="*100)
            print("\n‚ùå La requ√™te est vide ! Veuillez saisir une question ou des mots-cl√©s.\n")
            print("="*100 + "\n")
            return
        
        if not results['documents'][0]:
            print("\n" + "="*100)
            print(" üîç  RECHERCHE S√âMANTIQUE - AUCUN R√âSULTAT ".center(100))
            print("="*100)
            print(f"\nüí¨ Requ√™te : \"{query}\"")
            print(f"\n‚ùå Aucun r√©sultat trouv√© pour cette requ√™te.\n")
            print("="*100 + "\n")
            return
        
        print("\n" + "="*100)
        print(" üîç  RECHERCHE S√âMANTIQUE - R√âSULTATS ".center(100))
        print("="*100)
        print(f"\nüí¨ Requ√™te : \"{query}\"")
        print(f"üìä Nombre de r√©sultats trouv√©s : {len(results['documents'][0])}")
        print("\n" + "="*100 + "\n")
        
        # Parcourir tous les r√©sultats
        for i in range(1, 4):
            doc = results["documents"][0]
            metadata = results["metadatas"][0]
            distance = results["distances"][0]
            # Calculer le score de similarit√© (plus c‚Äôest proche de 100 %, mieux c‚Äôest)
            similarity_score = max(0, (2 - distance[i]) / 2 * 100)
            
            # D√©terminer l‚Äôemoji en fonction du score
            if similarity_score >= 70:
                score_emoji = "üü¢"
            elif similarity_score >= 40:
                score_emoji = "üü°"
            else:
                score_emoji = "üî¥"
            
            # Nettoyer le texte pour un meilleur affichage
            cleaned_doc = doc[i].replace('\\n', ' ').replace('\n', ' ')  # Remplace les retours √† la ligne
            cleaned_doc = ' '.join(cleaned_doc.split())  # Enl√®ve les espaces multiples
            
            print(f"‚ïî‚ïê üìÑ R√âSULTAT #{i} {'‚ïê'*85}")
            print(f"‚ïë")
            print(f"‚ïë  üìÇ Source      : {metadata[i].get('source', 'N/A')}")
            print(f"‚ïë  {score_emoji} Pertinence  : {similarity_score:.1f}%")
            print(f"‚ïë")
            print(f"‚ïë  üìù Extrait :")
            print(f"‚ïë  {'-'*96}")
            # Coupe le texte pour un affichage propre (75 caract√®res par ligne)
            words = cleaned_doc.split()
            line = "‚ïë  "
            for word in words:
                if len(line) + len(word) + 1 > 98:
                    print(line)
                    line = "‚ïë  " + word + " "
                else:
                    line += word + " "
            if line.strip() != "‚ïë":
                print(line)
            print(f"‚ïë")
            print(f"‚ïö{'‚ïê'*98}\n")

if __name__ == "__main__":
    # Initialise le pipeline de recherche
    retrieval_pipeline = RetrievalPipeline()
    
    print("üîÑ Indexation des documents...")

    # Parcourt tous les fichiers texte dans le dossier 'clean_data' et les indexe
    file_list = os.listdir("project\clean_data")

    for file_path in file_list:
        file_path = os.path.join("project\clean_data", file_path)
        retrieval_pipeline.index_text(file_path)

    # D√©finit une requ√™te de recherche (depuis la ligne de commande ou par d√©faut)
    if len(sys.argv) > 1:
        # R√©cup√®re la requ√™te depuis les arguments de la ligne de commande
        query = " ".join(sys.argv[1:])
    else:
        # Demande √† l‚Äôutilisateur de saisir une requ√™te
        print("\n" + "="*100)
        print(" üí¨  SAISISSEZ VOTRE REQU√äTE ".center(100))
        print("="*100)
        query = input("\nüîç Votre question : ").strip()
    
    # Ex√©cute la requ√™te sur la collection Chroma
    result = retrieval_pipeline.query_search(query)
    # Affiche les r√©sultats de recherche de mani√®re claire
    retrieval_pipeline.display_results(query, result)
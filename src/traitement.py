import chromadb 
from sentence_transformers import SentenceTransformer
import os
from pathlib import Path
import json
import re

class RetrievalPipeline:
    def __init__(self):
        # Initialise le mod√®le SentenceTransformer pour les embeddings de texte
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

        #base du projet ou ce fichier ce trouve
        self.base_dir = Path(__file__).resolve().parent
        self.project_root = self.base_dir.parent

        # chemin pour le dossier chroma
        self.db_path = (self.project_root / "data" / "chroma_db").resolve()
        # Cr√©e ou connecte une base de donn√©es Chroma persistante au chemin donn√©
        self.chroma_client = chromadb.PersistentClient(path=str(self.db_path))
        # R√©cup√®re ou cr√©e une collection dans la base appel√©e "law_text"
        self.collection = self.chroma_client.get_or_create_collection(name="law_text")

        #chemin vers clean_data
        self.data_dir = (self.project_root / "data" / "clean_data").resolve()
    
    def find_category(self, text):
        # trouve la categorie aproximatif
        
        json_path = (self.project_root / "data" / "base_dechets.json").resolve()
        # recupere le dic dans base_dechets.json
        with open(json_path, mode="r", encoding="utf-8") as f:
            category = json.load(f)
        # cree un dic avec les meme cle que l original
        dominent_category = {key:0 for key in category}
        # parcour chaque cle de category
        for key,data in category.items():
            # recupere les poid de chaque categorie
            weight = data["weight"]
            total = 0
            for word in data["keywords"]:
                #pour chaque partie de texte conte l'aparition des mot en ajoutent le poid
                pattern = r"\b" + word + r"\b"
                total += len(re.findall(pattern, text))
                dominent_category[key] = total * weight
        
        # la categorie qui apparait le plus
        dominent = max(dominent_category, key=dominent_category.get)
        # return la categorie
        return dominent

    def chunking(self, text, chunk_size=450, overlap=50):
        # Divise un texte long en petits segments qui se chevauchent pour une meilleure qualit√© d‚Äôembedding
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            # si le chunk est assez grand, on le garde
            if len(chunk) >= 250:
                chunks.append(chunk)

            # D√©place la fen√™tre vers l‚Äôavant, en gardant un chevauchement pour pr√©server le contexte
            start += chunk_size - overlap

        return chunks

    def index_text(self, file_path):
        
        # Lit le contenu du fichier texte en encodage UTF-8
        with open(file_path, "r", encoding='utf-8') as text:
            text_law = text.read()

        # Divise le texte en segments
        chunks = self.chunking(text_law)
        # R√©cup√®re le nom du fichier (sans extension) pour l‚Äôutiliser comme identifiant unique
        file_id = os.path.splitext(os.path.basename(file_path))[0]
        
        #essaye de recuperer la date
        pattern = r"(janv|fevr|mars|avr|mai|juin|juil|aout|sept|oct|nov|dec)[\s\-]+[0-9]{4}"
        # essaye de trouve une date au debut du texte
        match = re.search(pattern, text_law[:100], re.IGNORECASE)
        # si match = True return la date recupere
        if match:
            date = match.group(0)
        else:
            date="unknow"
        # R√©cup√®re les identifiants de documents existants dans la collection Chroma pour √©viter les doublons
        existing_ids = set(self.collection.get()["ids"])
        new_chunks = 0
        
        idx = len(existing_ids)

        # Boucle sur tous les segments du fichier
        for i, chunk in enumerate(chunks):
            idx += 1
            # Cr√©e un identifiant unique pour chaque segment bas√© sur le nom du fichier et son index
            chunk_id = f"{file_id}_chunk_{i}" 
            # recupere la categorie
            category = self.find_category(chunk)
            # Passe ce segment s‚Äôil est d√©j√† index√©
            if chunk_id in existing_ids:
                continue
            
            # G√©n√®re un embedding pour le segment √† l‚Äôaide du mod√®le
            embedding = self.model.encode(chunk, convert_to_numpy=True)
            # Ajoute le segment, son embedding et ses m√©tadonn√©es (chemin du fichier) √† la collection
            self.collection.add(
                ids=[chunk_id],
                documents=[chunk],
                embeddings=[embedding],
                metadatas=[{"source": file_id, "categorie": category, "date": date, "chunk_id":idx}]
            )
            new_chunks += 1

        # Affiche combien de nouveaux segments ont √©t√© index√©s
        if new_chunks > 0:
            print(f"{new_chunks} New chunk indexed from {file_path}")
    
    def query_search(self, query_text):
        # V√©rifie que la requ√™te n‚Äôest pas vide
        if not query_text or query_text.strip() == "":
            return None
        
        # Encode le texte de la requ√™te en un vecteur d‚Äôembedding
        query_embedding = self.model.encode(query_text)
        # Recherche dans la collection Chroma les segments les plus similaires
        n_result = 3
        result = self.collection.query(
            query_embeddings=[query_embedding],
            n_results= n_result
        )
        
        # list pour mettre les retour finaux des document
        final_result = []
        # reprend les metadata
        metadatas_dic = result["metadatas"][0]
        # parcour au nombre de resultat demander
        for i in range(n_result):
            # recupere les id des chunk
            metadata = metadatas_dic[i]
            idx = metadata["chunk_id"]
            # fait une recherche des chunk voisin 
            neighbors = self.collection.get(
                where={
                    "chunk_id":{"$in": [idx-1, idx, idx+1]}
                }
            )
            # ajoute les document trouver 
            doc_str = ""
            for doc in neighbors["documents"]:
                doc_str += str(doc)
            final_result.append(doc_str)
        # Retourne les r√©sultats de la recherche
        return final_result, result
    
    def display_results(self, query, response, results):
        """Affiche les r√©sultats de recherche de mani√®re claire et format√©e"""
        # V√©rifier si les r√©sultats sont valides
        if response is None:
            print("\n" + "="*100)
            print("   ERREUR ".center(100))
            print("="*100)
            print("\n La requ√™te est vide ! Veuillez saisir une question ou des mots-cl√©s.\n")
            print("="*100 + "\n")
            return
        
        if not response:
            print("\n" + "="*100)
            print("   RECHERCHE S√âMANTIQUE - AUCUN R√âSULTAT ".center(100))
            print("="*100)
            print(f"\n Requ√™te : \"{query}\"")
            print(f"\n Aucun r√©sultat trouv√© pour cette requ√™te.\n")
            print("="*100 + "\n")
            return
        
        print("\n" + "="*100)
        print("   RECHERCHE S√âMANTIQUE - R√âSULTATS ".center(100))
        print("="*100)
        print(f"\n Requ√™te : \"{query}\"")
        print(f" Nombre de r√©sultats trouv√©s : {len(response)}")
        print("\n" + "="*100 + "\n")
        
        # Parcourir tous les r√©sultats
        for i in range(0, 3):
            metadata = results["metadatas"][0]
            distance = results["distances"][0]
            # Calculer le score de similarit√© (plus c‚Äôest proche de 100 %, mieux c‚Äôest)
            similarity_score = max(0, (2 - distance[i]) / 2 * 100)
            
            # D√©terminer l‚Äôemoji en fonction du score
            if similarity_score >= 70:
                score_emoji = "üü¢"
            elif similarity_score >= 40:
                score_emoji = "üü°"
            else:
                score_emoji = "üî¥"
            
            # Nettoyer le texte pour un meilleur affichage
            cleaned_doc = response[i].replace('\\n', ' ').replace('\n', ' ')  # Remplace les retours √† la ligne
            cleaned_doc = ' '.join(cleaned_doc.split())  # Enl√®ve les espaces multiples
            
            print(f"‚ïî‚ïê  R√âSULTAT #{i} {'‚ïê'*85}")
            print(f"‚ïë")
            print(f"‚ïë   Source      : {metadata[i].get('source', 'N/A')}")
            print(f"‚ïë  {score_emoji} Pertinence  : {similarity_score:.1f}%")
            print(f"‚ïë")
            print(f"‚ïë   Extrait :")
            print(f"‚ïë  {'-'*96}")
            # Coupe le texte pour un affichage propre (75 caract√®res par ligne)
            words = cleaned_doc.split()
            line = "‚ïë  "
            for word in words:
                if len(line) + len(word) + 1 > 98:
                    print(line)
                    line = "‚ïë  " + word + " "
                else:
                    line += word + " "
            if line.strip() != "‚ïë":
                print(line)
            print(f"‚ïë")
            print(f"‚ïö{'‚ïê'*98}\n")

if __name__ == "__main__":
    # Initialise le pipeline de recherche
    retrieval_pipeline = RetrievalPipeline()
    
    print(" Indexation des documents...")
    
    # Parcourt tous les fichiers texte dans le dossier 'clean_data' et les indexe
    #base du projet ou ce fichier ce trouve
    base_dir = Path(__file__).resolve().parent
    project_root = base_dir.parent
    #chemin vers clean_data
    data_dir = (project_root / "data" / "clean_data").resolve()

    file_list = os.listdir(data_dir)

    for file in file_list:
        file_path = data_dir / file
        retrieval_pipeline.index_text(file_path)
    
    # Demande √† l‚Äôutilisateur de saisir une requ√™te
    print("\n" + "="*100)
    print("   SAISISSEZ VOTRE REQU√äTE ".center(100))
    print("="*100)
    query = input("\n Votre question : ").strip()
    
    # Ex√©cute la requ√™te sur la collection Chroma
    final_result, result_data = retrieval_pipeline.query_search(query)
    # Affiche les r√©sultats de recherche de mani√®re claire
    retrieval_pipeline.display_results(query, final_result, result_data)